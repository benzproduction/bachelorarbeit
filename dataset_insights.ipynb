{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General insights into the dataset\n",
    "\n",
    "### Answers to the following questions:\n",
    "- How many documents are in the dataset?\n",
    "- How many pages are there ?\n",
    "- How many pages on average ?\n",
    "- How many tokens (using the gpt tiktoken (izer)) are there ?\n",
    "- How many tokens on average ?\n",
    "- How many text chunks are there ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "txt_dir = 'data/raw/txt2'\n",
    "pdf_dir = 'data/raw/pdfs2'\n",
    "chunk_df_path = 'evaluation/data/chunks.csv'\n",
    "TEXT_EMBEDDING_CHUNK_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pdfs: 9\n",
      "Number of pages: 304\n",
      "Average number of pages per pdf: 33.77777777777778\n"
     ]
    }
   ],
   "source": [
    "# how many pages of average\n",
    "import PyPDF2\n",
    "pdf_files = sorted([x for x in os.listdir(pdf_dir) if 'DS_Store' not in x])\n",
    "\n",
    "pages_list = []\n",
    "pdf_count = 0\n",
    "for file in pdf_files:\n",
    "    pdfFileObj = open(os.path.join(pdf_dir,file), 'rb')\n",
    "    pdfReader = PyPDF2.PdfReader(pdfFileObj)\n",
    "    pages_list.append(len(pdfReader.pages))\n",
    "    pdf_count += 1\n",
    "\n",
    "print(f\"Number of pdfs: {pdf_count}\")\n",
    "print(f\"Number of pages: {sum(pages_list)}\")\n",
    "print(f\"Average number of pages per pdf: {sum(pages_list)/pdf_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text files: 9\n"
     ]
    }
   ],
   "source": [
    "txt_files = sorted([x for x in os.listdir(txt_dir) if 'DS_Store' not in x])\n",
    "print(f\"Number of text files: {len(txt_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 455\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def str_to_array_converter(string):\n",
    "    # Remove leading/trailing brackets and split the string by commas\n",
    "    elements = string[1:-1].split(',')\n",
    "    # Convert each element to float32 and create a numpy array\n",
    "    array = np.array(elements, dtype=np.float32)\n",
    "    return array\n",
    "\n",
    "textchunks_df = pd.read_csv(\"evaluation/data/chunks.csv\", converters={\"embedding\": str_to_array_converter})\n",
    "textchunks_df = textchunks_df[textchunks_df[\"text\"].notna()]\n",
    "\n",
    "print(f\"Number of text chunks: {len(textchunks_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2022-09-21_Immobilienmarkt_Deutschland_2022_2023_EN.pdf'\n",
      " '2022-12-12_ABG_Statements_Hoeller_2022-2023_en.pdf'\n",
      " '2023_Housing_market_outlook__Price_dip_and_interes.pdf'\n",
      " 'Deloitte 2023 Commercial Real Estate Outlook.pdf'\n",
      " 'Emerging-Trends_USCanada-2023.pdf'\n",
      " 'bouwinvest_international-market-outlook_2023-2025-1.pdf'\n",
      " 'global-real-estate-markets-2023.pdf' 'isa-outlook-2023.pdf'\n",
      " 'outlook-real-estatet-market-germany-dec-2022.pdf']\n"
     ]
    }
   ],
   "source": [
    "# unique filenames\n",
    "print(textchunks_df['filename'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def replace_semicolon(text, threshold=10):\n",
    "    '''\n",
    "    Get rid of semicolons.\n",
    "\n",
    "    First split text into fragments between the semicolons. If the fragment \n",
    "    is longer than the threshold, turn the semicolon into a period. O.w treat\n",
    "    it as a comma.\n",
    "\n",
    "    Returns new text\n",
    "    '''\n",
    "    new_text = \"\"\n",
    "    for subset in re.split(';', text):\n",
    "        subset = subset.strip() # Clear off spaces\n",
    "        # Check word count\n",
    "        if len(subset.split()) > threshold:\n",
    "            # Turn first char into uppercase\n",
    "            new_text += \". \" + subset[0].upper() + subset[1:]\n",
    "        else:\n",
    "            # Just append with a comma \n",
    "            new_text += \", \" + subset\n",
    "\n",
    "    return new_text\n",
    "\n",
    "USC_re = re.compile('[Uu]\\.*[Ss]\\.*[Cc]\\.]+') \n",
    "PAREN_re = re.compile('\\([^(]+\\ [^\\(]+\\)')\n",
    "BAD_PUNCT_RE = re.compile(r'([%s])' % re.escape('\"#%&\\*\\+/<=>@[\\]^{|}~_'), re.UNICODE)\n",
    "BULLET_RE = re.compile('\\n[\\ \\t]*`*\\([a-zA-Z0-9]*\\)')\n",
    "DASH_RE = re.compile('--+')\n",
    "WHITESPACE_RE = re.compile('\\s+')\n",
    "EMPTY_SENT_RE = re.compile('[,\\.]\\ *[\\.,]')\n",
    "FIX_START_RE = re.compile('^[^A-Za-z]*')\n",
    "FIX_PERIOD = re.compile('\\.([A-Za-z])')\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Borrowed from the FNDS text processing with additional logic added in.\n",
    "    Note: we do not take care of token breaking - assume a tokenizer\n",
    "    will handle this for us.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove parantheticals \n",
    "    text = PAREN_re.sub('', text)\n",
    "\n",
    "    # Get rid of enums as bullets or ` as bullets\n",
    "    text = BULLET_RE.sub(' ',text)\n",
    "    \n",
    "    # Clean html \n",
    "    text = text.replace('&lt;all&gt;', '')\n",
    "\n",
    "    # Remove annoying punctuation, that's not relevant\n",
    "    text = BAD_PUNCT_RE.sub('', text)\n",
    "\n",
    "    # Get rid of long sequences of dashes - these are formating\n",
    "    text = DASH_RE.sub( ' ', text)\n",
    "\n",
    "    # removing newlines, tabs, and extra spaces.\n",
    "    text = WHITESPACE_RE.sub(' ', text)\n",
    "    \n",
    "    # If we ended up with \"empty\" sentences - get rid of them.\n",
    "    text = EMPTY_SENT_RE.sub('.', text)\n",
    "    \n",
    "    # Attempt to create sentences from bullets \n",
    "    text = replace_semicolon(text)\n",
    "    \n",
    "    # Fix weird period issues + start of text weirdness\n",
    "    #text = re.sub('\\.(?=[A-Z])', '  . ', text)\n",
    "    # Get rid of anything thats not a word from the start of the text\n",
    "    text = FIX_START_RE.sub( '', text)\n",
    "    # Sometimes periods get formatted weird, make sure there is a space between periods and start of sent   \n",
    "    text = FIX_PERIOD.sub(\". \\g<1>\", text)\n",
    "\n",
    "    # Fix quotes\n",
    "    text = text.replace('``', '\"')\n",
    "    text = text.replace('\\'\\'', '\"')\n",
    "\n",
    "    # remove ambigous unicode characters\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 204023\n",
      "Average tokens per file: 22669.222222222223\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "token_count_list = []\n",
    "for file in txt_files:\n",
    "    with open(os.path.join(txt_dir,file), 'r') as f:\n",
    "        text = f.read()\n",
    "        text = clean_text(text)\n",
    "        tokens = tokenizer.encode(text)\n",
    "        token_count_list.append(len(tokens))\n",
    "\n",
    "print(f\"Number of tokens: {sum(token_count_list)}\")\n",
    "print(f\"Average tokens per file: {sum(token_count_list) / len(txt_files)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
