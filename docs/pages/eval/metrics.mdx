## Metrics for Information Retrieval

The Information Retriever in this QA system returns a fixed number of documents, which leads to a change in how the relevance of documents is evaluated. Relevance is determined by the information a document contributes to answering the posed question. The modified evaluation metrics are Precision and Normalized Discounted Cumulative Gain (NDCG).

The adapted NDCG metric assesses the quality of the ranking of the returned documents. Each relevant document is considered equally important. Let's define $D$ as the set of documents returned by the system, $n$ as the number of these documents, $R$ as the set of relevant documents, and $m$ is the number of these documents. A function $f(d)$ takes the value 1 if the document $d$ is relevant and 0 otherwise. With these definitions:

- DCG (Discounted Cumulative Gain) is defined as: 

    $DCG = \sum_{i=0}^{n-1} \frac{f(d)}{i+1}, \quad \text{for } d \in D$

- IDCG (Ideal Discounted Cumulative Gain) is defined as: 

    $IDCG = \sum_{i=0}^{m-1} \frac{1}{i+1}$

- nDCG (Normalized Discounted Cumulative Gain) is then calculated as:

    $nDCG = \frac{DCG}{IDCG}$

The modified Precision metric evaluates how well all relevant documents have been correctly returned by the retrieval system. Here the number of correctly returned documents is divided by the total number of documents that should have been returned. With the defined sets $D$ and $R$ and the function $f(d)$, the modified Precision can be defined as:

- Modified Precision = Number of true positives / Number of relevant documents
- $\text{Modified Precision} = \frac{\sum f(d)}{|R|}, \quad \text{for } d \in D$


## Metrics for the Answer Generator
To evaluate the answer generator within our system, two main metrics have been considered, designed to better reflect the capabilities of the answer generator in the context of factual questions.

The first metric verifies whether the Answer Generator can answer the question based on the provided information. For factual questions, a simple fact inclusion check using a Fuzzy Match is often sufficient. However, when it's not, a newly proposed metric, GPTScore, is used. GPTScore utilizes GPT models to evaluate generated texts based on specific criteria. For factual questions, the criteria have been defined as follows:

- The answer should include the key topic or topics presented in the ideal answer or question.
- The AI-generated answer should not omit the central fact or the main point of the ideal answer.
- The information provided in the AI answer should not contradict any of the details of the ideal answer.
- If the ideal answer or question contains specific statistics, data points, or percentages, these should be accurately represented in the AI-generated answer.
- For questions about economic or market indicators, the answer should accurately reflect the state or direction of these indicators as depicted in the ideal answer.

This metric is defined as a binary decision, returning either "factually matching" (1) or "factually contradicting" (0). This can be represented through the following function:

$
f(\text{Answer}) = 
\begin{cases}
1, & \text{if } (\text{Answer} \cap \text{Ideal Answer}) \neq \emptyset \text{ or criteria fulfilled}, \\
0, & \text{otherwise}
\end{cases}
$

where "Answer" is the response generated by the generator, and "Ideal Answer" is the expected response from the system.

Since GPTScore is a relatively new metric and the research community has not yet reached consensus on its effectiveness and reliability, if there's a discrepancy between the evaluations provided by GPTScore and Fuzzymatch, the generated answer is prompted for additional verification.

The second metric used evaluates the system's ability to extract key sentences from the provided sources and integrate them into the generated answer. To quantify this ability, BERTScore is used, a context-sensitive metric based on the BERT model. BERTScore calculates the cosine similarity between the context-sensitive embeddings of the generated and reference texts, producing a similarity measure that better reflects the semantic match of the texts than traditional n-gram-based metrics.

For each pair of generated sentence and reference sentence, the BERTScore is computed and stored in a matrix M with n rows and m columns, where n is the number of generated sentences and m is the number of reference sentences. The calculation of the BERTScore between a generated sentence $G_i$ and a reference sentence $R_j$ is represented by the formula $M_{ij} = BERTScore(G_i, R_j)$.

It's important that all relevant sentences are represented in the generated answer, so the metric chosen is the minimum of the maximum BERTScores for each reference sentence. This is mathematically represented by $min_{j∈[1,m]}(max_{i∈[1,n]}(M_{ij}))$. This metric emphasizes the worst match and ensures that the worst representation of a relevant sentence in the generated answer is highlighted.

A threshold of 0.85 is set to determine whether a relevant sentence is adequately represented in the generated answer. The choice of this threshold is a compromise between the need to ensure high semantic matches and the recognition that perfect matches can be rare due to differences in phrasing and the system's use of paraphrasing. A score of 0.85 or higher is considered a high semantic match, based on empirical observations by Zhang et al. (2020) in their evaluation of the BERTScore metric.

To assess the overall quality of the generated answer, the minimum of the maximum BERTScores for all relevant sentences is calculated. If this minimum exceeds the threshold of 0.85, it can be assumed that the generated answer adequately represents all relevant sentences from the provided sources.