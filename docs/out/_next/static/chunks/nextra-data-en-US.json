{"/app":{"title":"Application Overview","data":{"":"","features#Features":""}},"/eval/metrics":{"title":"Metrics","data":{"":""}},"/eval/results":{"title":"Results","data":{"":""}},"/eval":{"title":"Eval","data":{"":"","methodology#Methodology":"The system presented in this repo aims to extract meaningful content from an arbitrary number of reference documents and consequently answer a question. The goal is to ensure that the generated answer is in line with the reference documents in terms of coherence, readability, conciseness, and factual accuracy. The evaluation of such a system can therefore be divided into (i) extraction of relevant content and (ii) generation of the answer. It is thus recommended to evaluate these two tasks separately and finally in cooperation also to assess the (iii) overall performance of the question-answer system. The following will detail these three evaluation points.","setup#Setup":"The scripts and components provided allow for a modular and customizable evaluation of the question-answer system. This allows for fine-tuning and improvements as needed, based on the evaluation results.\nFor a comprehensive evaluation and the ability to quickly adjust to any changes that may have to be made, the evaluation is structured in three main components:\nComponent Classes\nRegistry Class\nEvalRun Class","component-classes#Component Classes":"The component classes are divided into three base components, each fulfilling a specific role in the system:\nEmbedding Model: This class converts a given string into a vector. The method implementing this function must conform to the EmbeddingsFn signature and is configured to take in the string to be embedded as an argument and return a list of floats representing the embedded vector.\nInformation Retriever: This class's task is to find k similar documents to a given query. It must conform to the RetrieverFn signature and is configured to receive a query and an EmbeddingsFn object to be used for embedding the query. It returns a pandas dataframe representing the retrieved documents and its metadata (e.g. filename, id, vector score).\nAnswer Generator (e.g., Language Model): This class generates responses based on a given prompt, a query, and optional sources. It must conform to the CompletionFn signature and is configured to receive a prompt template, a query, and optionally an EmbeddingsFn and a RetrieverFn object. It returns a CompletionResult object containing the generated completion.","registry-class#Registry Class":"The Registry class handles component configurations. It loads and creates components defined in configuration files in the registry folder. When a user wants to use a component class in an evaluation configuration, they can simply adjust the corresponding configuration file in the registry folder to specify the desired class and additional parameters required to initialize the class. The Registry class is used at runtime to instantiate and execute the class defined in the configuration file.Example from evaluation/registry/llm/text-davinci-003.yaml:\ntext-davinci-003:\nclass: components.llms:OpenAICompletionFn\nargs:\ndeployment_name: davinci\napi_key: ${OPENAI_API_KEY}\napi_base: ${AZURE_OPENAI_ENDPOINT}\nextra_options:\ntemperature: 0\nn: 1\nmax_tokens: 300","evalrun-class#EvalRun Class":"The EvalRun class is responsible for conducting the actual evaluation. It receives from the run.py script a RunSpec that includes the executable classes loaded via the Registry and the corresponding dataset. The EvalRun class then iterates over the test samples and calculates corresponding metrics. The results are saved in a JSONL file, so the results can be traced and analyzed.If other metrics are to be used, it is necessary to adjust the EvalRun class and, in particular, the run function.","running-an-evaluation#Running an Evaluation":"To conduct an evaluation, run the run.py script. This script will prompt the user with a series of questions, guiding them through the setup process for the evaluation. This includes specifying the type of evaluation, selecting the desired configuration for the evaluation from the registry folder, and choosing the test dataset.Following the prompts and making the appropriate selections will initiate the evaluation, with results stored in the evaluation/runs folder for review and analysis. This setup allows for quick adjustments to various components and configurations without the need for extensive code changes or adjustments to previous configurations."}},"/":{"title":"Introduction","data":{"":"This project, developed as part of my bachelor's thesis, aims at creating an intelligent question-answer system for searching and retrieving information in file data spaces. The repository is split into two main parts: the application and the evaluation of the system. The application itself comes in two versions: one built with Streamlit (Python-based), and another built with Next.js. To test the application locally, it's recommended to use the Streamlit version.","getting-started#Getting Started":"Before you can start using the application, there are a few steps you need to follow:","downloading-the-used-documents#Downloading the Used Documents":"Documents can be downloaded by running the Python script found in data/load.py. This will download the PDFs and store them in the data/raw/pdfs directory.","docker-startup#Docker Startup":"The application requires a running Redis Stack on localhost:6379. This can be easily set up by running the following Docker command:\ndocker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest","environment-variables#Environment Variables":"The application requires two environment variables to be set:\nOPENAI_API_KEY: Your OpenAI API key.\nAZURE_OPENAI_ENDPOINT: The endpoint for Azure's OpenAI service.\n\nThe application was developed using the Azure OpenAI language model Text-Davinci-003. Using the Azure version is not strictly required, but it is recommended as changing to the standard OpenAI API would require modifications to the code in multiple places und may change the evaluation results.","overview-of-the-general-idea#Overview of the General Idea":"The goal of this project was to design and implement a system that can effectively search and retrieve information from a data space made up of files. The challenge was to enable users to intuitively find information without having to know where and how it's stored. The project addresses this challenge by leveraging artificial intelligence technologies, specifically language models and vector databases.The developed system works as follows:\nText Segmentation and Vectorization: A collection of PDF documents is divided into individual text segments, known as \"chunks\". These text chunks are then converted into vectors using an embedding model.\nStorage in Database: The resulting vectors are stored in a Redis database.\nQuery Processing: When a search query is made, the query itself is also converted into a vector and sent to the database.\nSearching for Similar Text Segments: The database performs a k-nearest neighbors (kNN) search and returns the best matching text chunks.\nGeneration of Response: The language model then takes the query (question) and the matching text chunks, and creates a summary in the context of the query. This summary is then presented to the user as a concrete answer.\n\nThe result is an intelligent question-answer system that allows users to search and retrieve information in an intuitive and user-friendly manner."}}}