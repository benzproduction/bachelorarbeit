{"/app":{"title":"Application Overview","data":{"":"","features#Features":""}},"/eval/results":{"title":"Results","data":{"":"","results-of-the-baseline-configuration#Results of the Baseline configuration":"Baseline Configuration:\nbaseline:\nembedding: text_embedding_ada_002\nlanguage_model: text-davinci-003\nretriever: redis\nprompt: baseline\nrun_args:\nk: 7\n\nNCDG\tPrecision\tCorrect\tBERT\tCorrect\tSingle Doc with high word similarity\t0.496\t0.8\t1.0\t0.9\t0.8\tMultiple Docs with high word similarity\t0.451\t0.617\t0.9\t0.857\t1.0\tSingle Doc with low word similarity\t0.486\t0.733\t1.0\t0.894\t0.667\tMulitple Docs with low word similarity\t0.402\t0.517\t0.933\t0.854\t0.9\tQuestions unrelated to the docs\t-\t-\t-\t-\t0.167\t---\t---\t---\t---\t---\t---\tOverall\t0.459\t0.669\t0.958\t0.876\t0.707"}},"/eval/metrics":{"title":"Metrics","data":{"":"","metrics-for-information-retrieval#Metrics for Information Retrieval":"The Information Retriever in this QA system returns a fixed number of documents, which leads to a change in how the relevance of documents is evaluated. Relevance is determined by the information a document contributes to answering the posed question. The modified evaluation metrics are Precision and Normalized Discounted Cumulative Gain (NDCG).The adapted NDCG metric assesses the quality of the ranking of the returned documents. Each relevant document is considered equally important. Let's define  as the set of documents returned by the system,  as the number of these documents,  as the set of relevant documents, and  is the number of these documents. A function  takes the value 1 if the document  is relevant and 0 otherwise. With these definitions:\nDCG (Discounted Cumulative Gain) is defined as:\nIDCG (Ideal Discounted Cumulative Gain) is defined as:\nnDCG (Normalized Discounted Cumulative Gain) is then calculated as:\n\nThe modified Precision metric evaluates how well all relevant documents have been correctly returned by the retrieval system. Here the number of correctly returned documents is divided by the total number of documents that should have been returned. With the defined sets  and  and the function , the modified Precision can be defined as:\nModified Precision = Number of true positives / Number of relevant documents","metrics-for-the-answer-generator#Metrics for the Answer Generator":"To evaluate the answer generator within our system, two main metrics have been considered, designed to better reflect the capabilities of the answer generator in the context of factual questions.The first metric verifies whether the Answer Generator can answer the question based on the provided information. For factual questions, a simple fact inclusion check using a Fuzzy Match is often sufficient. However, when it's not, a newly proposed metric, GPTScore, is used. GPTScore utilizes GPT models to evaluate generated texts based on specific criteria. For factual questions, the criteria have been defined as follows:\nThe answer should include the key topic or topics presented in the ideal answer or question.\nThe AI-generated answer should not omit the central fact or the main point of the ideal answer.\nThe information provided in the AI answer should not contradict any of the details of the ideal answer.\nIf the ideal answer or question contains specific statistics, data points, or percentages, these should be accurately represented in the AI-generated answer.\nFor questions about economic or market indicators, the answer should accurately reflect the state or direction of these indicators as depicted in the ideal answer.\n\nThis metric is defined as a binary decision, returning either \"factually matching\" (1) or \"factually contradicting\" (0). This can be represented through the following function:where \"Answer\" is the response generated by the generator, and \"Ideal Answer\" is the expected response from the system.Since GPTScore is a relatively new metric and the research community has not yet reached consensus on its effectiveness and reliability, if there's a discrepancy between the evaluations provided by GPTScore and Fuzzymatch, the generated answer is prompted for additional verification.The second metric used evaluates the system's ability to extract key sentences from the provided sources and integrate them into the generated answer. To quantify this ability, BERTScore is used, a context-sensitive metric based on the BERT model. BERTScore calculates the cosine similarity between the context-sensitive embeddings of the generated and reference texts, producing a similarity measure that better reflects the semantic match of the texts than traditional n-gram-based metrics.For each pair of generated sentence and reference sentence, the BERTScore is computed and stored in a matrix M with n rows and m columns, where n is the number of generated sentences and m is the number of reference sentences. The calculation of the BERTScore between a generated sentence  and a reference sentence  is represented by the formula .It's important that all relevant sentences are represented in the generated answer, so the metric chosen is the minimum of the maximum BERTScores for each reference sentence. This is mathematically represented by . This metric emphasizes the worst match and ensures that the worst representation of a relevant sentence in the generated answer is highlighted.A threshold of 0.85 is set to determine whether a relevant sentence is adequately represented in the generated answer. The choice of this threshold is a compromise between the need to ensure high semantic matches and the recognition that perfect matches can be rare due to differences in phrasing and the system's use of paraphrasing. A score of 0.85 or higher is considered a high semantic match, based on empirical observations by Zhang et al. (2020) in their evaluation of the BERTScore metric.To assess the overall quality of the generated answer, the minimum of the maximum BERTScores for all relevant sentences is calculated. If this minimum exceeds the threshold of 0.85, it can be assumed that the generated answer adequately represents all relevant sentences from the provided sources."}},"/":{"title":"Introduction","data":{"":"This project, developed as part of my bachelor's thesis, aims at creating an intelligent question-answer system for searching and retrieving information in file data spaces. The repository is split into two main parts: the application and the evaluation of the system. The application itself comes in two versions: one built with Streamlit (Python-based), and another built with Next.js. To test the application locally, it's recommended to use the Streamlit version.","getting-started#Getting Started":"Before you can start using the application, there are a few steps you need to follow:","downloading-the-used-documents#Downloading the Used Documents":"Documents can be downloaded by running the Python script found in data/load.py. This will download the PDFs and store them in the data/raw/pdfs directory.","docker-startup#Docker Startup":"The application requires a running Redis Stack on localhost:6379. This can be easily set up by running the following Docker command:\ndocker run -d --name redis-stack -p 6379:6379 -p 8001:8001 redis/redis-stack:latest","environment-variables#Environment Variables":"The application requires two environment variables to be set:\nOPENAI_API_KEY: Your OpenAI API key.\nAZURE_OPENAI_ENDPOINT: The endpoint for Azure's OpenAI service.\n\nThe application was developed using the Azure OpenAI language model Text-Davinci-003. Using the Azure version is not strictly required, but it is recommended as changing to the standard OpenAI API would require modifications to the code in multiple places und may change the evaluation results.","overview-of-the-general-idea#Overview of the General Idea":"The goal of this project was to design and implement a system that can effectively search and retrieve information from a data space made up of files. The challenge was to enable users to intuitively find information without having to know where and how it's stored. The project addresses this challenge by leveraging artificial intelligence technologies, specifically language models and vector databases.The developed system works as follows:\nText Segmentation and Vectorization: A collection of PDF documents is divided into individual text segments, known as \"chunks\". These text chunks are then converted into vectors using an embedding model.\nStorage in Database: The resulting vectors are stored in a Redis database.\nQuery Processing: When a search query is made, the query itself is also converted into a vector and sent to the database.\nSearching for Similar Text Segments: The database performs a k-nearest neighbors (kNN) search and returns the best matching text chunks.\nGeneration of Response: The language model then takes the query (question) and the matching text chunks, and creates a summary in the context of the query. This summary is then presented to the user as a concrete answer.\n\nThe result is an intelligent question-answer system that allows users to search and retrieve information in an intuitive and user-friendly manner."}},"/eval":{"title":"Eval","data":{"":"","methodology#Methodology":"The system presented in this repo aims to extract meaningful content from an arbitrary number of reference documents and consequently answer a question. The goal is to ensure that the generated answer is in line with the reference documents in terms of coherence, readability, conciseness, and factual accuracy. The evaluation of such a system can therefore be divided into (i) extraction of relevant content and (ii) generation of the answer. It is thus recommended to evaluate these two tasks separately and finally in cooperation also to assess the (iii) overall performance of the question-answer system. The following will detail these three evaluation points.","setup#Setup":"The scripts and components provided allow for a modular and customizable evaluation of the question-answer system. This allows for fine-tuning and improvements as needed, based on the evaluation results.\nFor a comprehensive evaluation and the ability to quickly adjust to any changes that may have to be made, the evaluation is structured in three main components:\nComponent Classes\nRegistry Class\nEvalRun Class","component-classes#Component Classes":"The component classes are divided into three base components, each fulfilling a specific role in the system:\nEmbedding Model: This class converts a given string into a vector. The method implementing this function must conform to the EmbeddingsFn signature and is configured to take in the string to be embedded as an argument and return a list of floats representing the embedded vector.\nInformation Retriever: This class's task is to find k similar documents to a given query. It must conform to the RetrieverFn signature and is configured to receive a query and an EmbeddingsFn object to be used for embedding the query. It returns a pandas dataframe representing the retrieved documents and its metadata (e.g. filename, id, vector score).\nAnswer Generator (e.g., Language Model): This class generates responses based on a given prompt, a query, and optional sources. It must conform to the CompletionFn signature and is configured to receive a prompt template, a query, and optionally an EmbeddingsFn and a RetrieverFn object. It returns a CompletionResult object containing the generated completion.","registry-class#Registry Class":"The Registry class handles component configurations. It loads and creates components defined in configuration files in the registry folder. When a user wants to use a component class in an evaluation configuration, they can simply adjust the corresponding configuration file in the registry folder to specify the desired class and additional parameters required to initialize the class. The Registry class is used at runtime to instantiate and execute the class defined in the configuration file.Example from evaluation/registry/llm/text-davinci-003.yaml:\ntext-davinci-003:\nclass: components.llms:OpenAICompletionFn\nargs:\ndeployment_name: davinci\napi_key: ${OPENAI_API_KEY}\napi_base: ${AZURE_OPENAI_ENDPOINT}\nextra_options:\ntemperature: 0\nn: 1\nmax_tokens: 300","evalrun-class#EvalRun Class":"The EvalRun class is responsible for conducting the actual evaluation. It receives from the run.py script a RunSpec that includes the executable classes loaded via the Registry and the corresponding dataset. The EvalRun class then iterates over the test samples and calculates corresponding metrics. The results are saved in a JSONL file, so the results can be traced and analyzed.If other metrics are to be used, it is necessary to adjust the EvalRun class and, in particular, the run function.","running-an-evaluation#Running an Evaluation":"To conduct an evaluation, run the run.py script. This script will prompt the user with a series of questions, guiding them through the setup process for the evaluation. This includes specifying the type of evaluation, selecting the desired configuration for the evaluation from the registry folder, and choosing the test dataset.Following the prompts and making the appropriate selections will initiate the evaluation, with results stored in the evaluation/runs folder for review and analysis. This setup allows for quick adjustments to various components and configurations without the need for extensive code changes or adjustments to previous configurations."}}}